---
date: "2022-02-12T00:00:00Z"
external_link: ""
image:
  caption: Photo from The Church of Jesus Christ of Latter-Day Saints
  focal_point: Smart
summary: Webscraping and analysis of text from talks from General Conferences of the Church of Jesus Christ of Latter-Day Saints with a Shiny app to explore the talks.
tags:
- Text analysis
- Text processing
- Webscraping
- Shiny
title: Text Analysis and Processing of General Conference Talks from The Church of Jesus Christ of Latter-Day Saints
author: David Teuscher
links: 
 - names: "GitHub"
   url: https://github.com/skylerg022/conference-church-of-jesus-christ
   icon_pack: fab
   icon: github
 - names: "Shiny"
   url: https://david-teuscher.shinyapps.io/conference-analysis-app/
   icon_pack: fab
   icon: r-project
    
url_code: ""
url_pdf: ""
url_slides: ""
url_video: ""
---



<p>This project takes text from talks from General Conferenece of the Church of Latter-Day Saints and processes and analyzes the text. The processed data can be explored with this <a href="https://david-teuscher.shinyapps.io/conference-analysis-app/">Shiny app</a>. The code for scraping the data and the analysis can be found <a href="https://github.com/skylerg022/conference-church-of-jesus-christ">here</a>.</p>
<p>Every 6 months The Church of Jesus Christ of Latter-Data Saints holds their General Conference, where church leaders speak for approximately 10 hours of the course of two days. The messages vary from speaker to speaker and from conference to conference. The text for these talks are available from as early as April 1971 through the most recent conference in October 2021. With the large amount of text from a variety of speakers, I was interested in how the usage of words and topics changed over time as well as common words for each speaker and conference.</p>
<p>The project is a combination for work from <a href="https://github.com/skylerg022">Skyler Gray</a>, <a href="https://github.com/GarrettDaniel">Daniel Garrett</a>, <a href="https://github.com/germckay">Mckay Gerratt</a>, and myself.</p>
<p>The data was web scraped from the official site of The Church of Jesus Christ of Latter-Day Saints using Beautiful Soup and Selenium in Python. There is a page with a similar format for each conference. An example of the page to be scraped is <a href="https://www.churchofjesuschrist.org/study/general-conference/1971/04?lang=eng">here</a>. On this page, it lists the title for each talk, the speaker’s name, and possibly a small excerpt from the talk. For each talk, we extracted the speaker’s name as well as the <code>href</code> elements that corresponded to the link to the actual talk.</p>
<p>After collecting all of the links, we looped through all of the links and pulled the title of the talk and the text of the talk. The resulting information was saved into a pandas DataFrame that contained the year and month of the conference, the speaker’s name, the talk title, and the text of the talk. Afterwards, the data was written to a .csv file to be used in R for further analysis. The Python code for this can be found <a href="https://github.com/skylerg022/conference-church-of-jesus-christ/blob/master/conference_scraper.ipynb">here</a>.</p>
<p>After scraping the text of the talks, the text was processed and a Shiny app was developed to look at some of the patterns over time and by speaker. The text was tokenized by splitting the text into words and then stopwords, punctuation and numbers were removed from the text using the <code>tidytext</code> package. An example of the code used is shown below:</p>
<pre class="r"><code>tidy_conf &lt;- conf %&gt;%
    filter(!is.na(text)) %&gt;%
    arrange(desc(year), desc(month)) %&gt;%
    unnest_tokens(word, text) %&gt;%
    anti_join(stop_words) %&gt;%
    filter(!str_detect(word, &#39;[0-9]+(:[0-9]+)*&#39;))</code></pre>
<p>Once the text was in this format, it was summarized in a few different ways:</p>
<ol style="list-style-type: decimal">
<li>The times a word was used was calculated for each speaker</li>
<li>The times a word was used over time (showing the change in word usage over the years)</li>
<li>A word cloud for a single conference was created</li>
</ol>
<p>A tab for each of these different summaries was included in the Shiny app. The app allows you to choose among all speakers who have spoken during General Conference and any word can be searched to see its usage over time. The figures below show an example of the frequency plot and the change in words over time.</p>
<p><img src="Figures/frequency.jpeg" />
<img src="Figures/word-time.jpeg" /></p>
<p>In addition to the texts of the talks, there are tags on each talk associated with a specific topic. For example, a talk may have tags such as faith, Jesus Christ, revelation, etc. The picture below lists some other examples of topics that exist for these talks. Selenium was used to scrape the talks as each topic was selected and then all of the talks and information about the talks were scraped. The HTML setup of the website made it easy to obtain this information by specifying a specific HTML class. The talk name, speaker, month, year, and topic were all included as an observation in the data that was saved.</p>
<p><img src="Figures/topic-example.jpeg" /></p>
<p>With the topic information, similar summaries were created for this data, with the times a topic was talked about was calculated for each speaker and the times a topic was spoken about over time. The Shiny app allows a user to select to examine the topic summaries or word summaries.</p>
<p>Currently, most of this work includes scraping text data from the internet and processing it to summarize basic information about the text for each speaker. It would be interesting to do further analysis, such as sentiment analysis, part-of-speech tagging, or topic modeling as a few examples. With text data, there are a number of ways to explore and analysis the data and this work only scrapes the surface of what can be done with text data.</p>
<p>While this specific example (talks from General Conference) may be interesting to only a specific group of people, the process that was applied could easily be done to any type of texts, especially something that occurs consistently each year. The most difficult part of the process is understanding the layout of the website where the texts are contained. Once the HTML structure is understood, the process for scraping the data and summarizing it would be similar to the process that is taken here.</p>
<p>If there are any questions or feedback about the process or analysis or the development of the Shiny app, feel free to reach out to me via email.</p>
